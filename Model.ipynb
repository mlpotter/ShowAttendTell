{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    embedding_size = m\n",
    "    vocabulary_size (number of unique words) = K\n",
    "    hidden_size (LSTM dimensionality) = n\n",
    "    context_size (context vector size) = D\n",
    "    num_context_vec (number of context vectors) = L\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,vocabulary_size,embedding_size=100,hidden_size=1000,context_size=512,num_context_vec=14*14):\n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        self.embedding_size=embedding_size\n",
    "        self.vocabulary_size=vocabulary_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.context_size = context_size\n",
    "        self.num_context_vec = num_context_vec\n",
    "        \n",
    "        \n",
    "        # Embedding matrices\n",
    "        self.Lo = nn.Linear(embedding_size,vocabulary_size)\n",
    "        self.Lh = nn.Linear(hidden_size,embedding_size)\n",
    "        self.Lz = nn.Linear(context_size,embedding_size)\n",
    "        self.E = nn.Linear(vocabulary_size,embedding_size)\n",
    "        print(\"Initialized embedding matrices for p(y | a, y_prev)\")\n",
    "    \n",
    "    \n",
    "        # initial memory state and hidden state\n",
    "        self.f_init_c = nn.Linear(context_size,hidden_size)\n",
    "        self.f_init_h = nn.Linear(context_size,hidden_size)\n",
    "        print(\"Initialized memory state and hidden state fc layers for LSTM\")\n",
    "        \n",
    "        # soft attention\n",
    "        self.f_att = nn.Linear(hidden_size+context_size,1)\n",
    "        print(\"Initialized soft version of attention mechanism\")\n",
    "        \n",
    "        # Beta for object focus\n",
    "        self.gate_scalar = nn.Linear(hidden_size,1)\n",
    "        print(\"Beta Initialized\")\n",
    "    \n",
    "        self.LSTM = nn.LSTM(input_size=context_size+hidden_size+embedding_size,hidden_size=hidden_size,batch_first=True)\n",
    "        print(\"Initialized LSTM\")\n",
    "        \n",
    "    def init_hidden(self,a):\n",
    "        c0 = self.f_init_c(torch.mean(a,dim=2).unsqueeze(dim=1))\n",
    "        h0 = self.f_init_h(torch.mean(a,dim=2).unsqueeze(dim=1))\n",
    "        return h0,c0\n",
    "        \n",
    "    def forward(self,a_i,input,hn_prev,cn_prev):\n",
    "        # initial hidden state and cell state\n",
    "        \n",
    "        # attention model biased on previous hidden state\n",
    "        e_ti = []\n",
    "        for i in range(self.num_context_vec):\n",
    "            e_ti.append(self.f_att(torch.cat( (a_i[:,:,i].unsqueeze(1),hn_prev) ,dim=2)))\n",
    "        e_ti = torch.cat(e_ti,dim=2)\n",
    "\n",
    "        alpha_ti = torch.softmax(e_ti,dim=2)\n",
    "        \n",
    "        # context vector\n",
    "        #print(alpha_ti.size())\n",
    "        #print(a_i.size())\n",
    "        beta = torch.sigmoid( self.gate_scalar(hn_prev) )\n",
    "        z_expectation = beta * torch.sum(alpha_ti*a_i,dim=2).unsqueeze(dim=1)\n",
    "        #print(z_expectation.size())\n",
    "        #print(hn_prev.size())\n",
    "        # word embedding\n",
    "        w_embedding = self.E(input)\n",
    "        #print(w_embedding.size())\n",
    "        lstm_input = torch.cat( (w_embedding,hn_prev,z_expectation) ,dim=2)\n",
    "        #print(lstm_input.size())\n",
    "        \n",
    "        _,(hn,cn) = self.LSTM(lstm_input,(hn_prev.squeeze(1).unsqueeze(0),cn_prev.squeeze(1).unsqueeze(0)))\n",
    "        # nt\n",
    "        #print(self.E(input).size())\n",
    "        #print(self.Lh(h0).size())\n",
    "        #print(self.Lz(z_expectation).size())\n",
    "        #p_yt = torch.softmax( torch.exp( self.Lo( w_embedding + self.Lh(hn.squeeze(0).unsqueeze(1)) + self.Lz(z_expectation) ) ) ,dim=2)\n",
    "        p_yt = self.Lo( w_embedding + self.Lh(hn.squeeze(0).unsqueeze(1)) + self.Lz(z_expectation) )\n",
    "\n",
    "        #print(p_yt.size())\n",
    "        \n",
    "        return p_yt,hn.squeeze(0).unsqueeze(1),cn.squeeze(0).unsqueeze(1),alpha_ti\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Dimension information:\\nm={}\\nK={}\\nn={}\\nD={}L={}\\n\".format(self.embedding_dim,self.vocabulary_size,self.hidden_size,self.context_size)\n",
    "\n",
    "# In[7]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG:\n",
    "    def __init__(self):\n",
    "        self.vgg19 = models.vgg19(pretrained=True).features[:35]\n",
    "        \n",
    "    def __call__(self,input):\n",
    "        with torch.no_grad():\n",
    "            return self.vgg19(input).view(-1,512,14*14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized embedding matrices for p(y | a, y_prev)\n",
      "Initialized memory state and hidden state fc layers for LSTM\n",
      "Initialized soft version of attention mechanism\n",
      "Beta Initialized\n",
      "Initialized LSTM\n",
      "torch.Size([2, 1, 196])\n",
      "torch.Size([2, 1, 512])\n",
      "torch.Size([2, 1, 64])\n",
      "torch.Size([2, 1, 200])\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)\n",
    "from PIL import Image\n",
    "\n",
    "input_image = Image.open(filename)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "lstm = LSTM(128,200,64,512,14*14)\n",
    "vgg = VGG()\n",
    "ai = vgg(torch.cat( (input_batch,input_batch),0) )\n",
    "\n",
    "c0,h0 = lstm.init_hidden(ai)\n",
    "yt = torch.randn((2,1,lstm.vocabulary_size))\n",
    "f = lstm.forward(ai,yt,c0,h0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
