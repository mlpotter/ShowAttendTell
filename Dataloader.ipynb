{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flickr8000(Dataset):\n",
    "    def __init__(self,IMAGE_PATH=r'dataset\\images',TEXT_PATH=r'dataset\\text'):\n",
    "        \n",
    "        self.IMAGE_PATH = IMAGE_PATH\n",
    "        self.TEXT_PATH = TEXT_PATH\n",
    "        \n",
    "        self.dataset = pd.read_csv(os.path.join(self.TEXT_PATH,'captions.txt'))\n",
    "        self.len = self.dataset.shape[0]\n",
    "\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        self.dictionary = dict.fromkeys(sorted(list(set(\" \".join(self.dataset['caption'].values.tolist()).split()))))\n",
    "        \n",
    "        for i,word in enumerate(list(self.dictionary.keys())):\n",
    "            one_hot_encoding = torch.zeros((1,len(self.dictionary)))\n",
    "            one_hot_encoding[0,i] = 1\n",
    "            self.dictionary[word] = one_hot_encoding\n",
    "            \n",
    "        self.dictionary_len = {}\n",
    "        for i,text in enumerate(self.dataset['caption']):\n",
    "            if text[-1] != \".\":\n",
    "                text = text + \" .\"\n",
    "            text_size = len(text.split())\n",
    "            if text_size not in self.dictionary_len:\n",
    "                self.dictionary_len[text_size] = [i]\n",
    "            else:\n",
    "                self.dictionary_len[text_size].append(i)\n",
    "                                \n",
    "    def __getitem__(self,index):\n",
    "        indices = self.dictionary_len[index]\n",
    "        \n",
    "        #image \n",
    "        image = Image.open(os.path.join(self.IMAGE_PATH,self.dataset.iloc[indices[0]]['image']))\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image).unsqueeze(0)\n",
    "        \n",
    "        # text   \n",
    "        text = self.dataset.iloc[indices[0]]['caption']\n",
    "        if text[-1] != \".\":\n",
    "            text = text + \" .\"    \n",
    "        text = text.split()\n",
    "        text_encoded = self.dictionary[text[0]]\n",
    "        for word in text[1:]:\n",
    "            text_encoded = torch.cat((text_encoded,self.dictionary[word]),0)\n",
    "        caption = text_encoded.unsqueeze(0)\n",
    "                    \n",
    "        for idx in indices[1:]:\n",
    "            # image\n",
    "            img = Image.open(os.path.join(self.IMAGE_PATH,self.dataset.iloc[idx]['image'])) \n",
    "            if self.transforms:\n",
    "                img = self.transforms(img).unsqueeze(0)\n",
    "            image = torch.cat( (image,img) ,0)\n",
    "            \n",
    "            # text\n",
    "            text = self.dataset.iloc[idx]['caption']\n",
    "            if text[-1] != \".\":\n",
    "                text = text + \" .\"\n",
    "            text = text.split()\n",
    "            text_encoded = self.dictionary[text[0]]\n",
    "            for word in text[1:]:\n",
    "                text_encoded = torch.cat((text_encoded,self.dictionary[word]),0)\n",
    "            text_encoded.unsqueeze_(0)\n",
    "            caption = torch.cat( (caption,text_encoded) ,0)\n",
    "        \n",
    "        print(int(len(indices)/index))\n",
    "        print(len(indices))\n",
    "        print(index)\n",
    "        caption_labels = torch.cat((caption != 0).nonzero()[:,2:3].chunk(int(len(indices))),1).transpose(0,1)\n",
    "        \n",
    "        return image,caption,caption_labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flickr8000_subset(Dataset):\n",
    "    def __init__(self,IMAGE_PATH=r'dataset\\images',TEXT_PATH=r'dataset\\text',sentence_len=11):\n",
    "        \n",
    "        self.IMAGE_PATH = IMAGE_PATH\n",
    "        self.TEXT_PATH = TEXT_PATH\n",
    "        \n",
    "        self.dataset = pd.read_csv(os.path.join(self.TEXT_PATH,'captions.txt'))\n",
    "        # fix the end character to always be \".\"\n",
    "        self.dataset['caption'] = self.dataset['caption'].apply(lambda text: text+\" .\" if text[-1] != \".\" else text)\n",
    "        # split the caption into a list of words\n",
    "        self.dataset['caption'] = self.dataset['caption'].apply(str.split)\n",
    "        # keep only caption and images of sentence length specified\n",
    "        self.dataset = self.dataset[self.dataset['caption'].apply(len)==sentence_len].reset_index(drop=True)\n",
    "\n",
    "        self.len = self.dataset.shape[0]\n",
    "        self.sentence_len = sentence_len\n",
    "\n",
    "        self.transforms =  transforms.Compose([\n",
    "            transforms.Resize((224,224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "        self.dictionary = dict.fromkeys(sorted(list(set(\" \".join(self.dataset['caption'].apply(\" \".join).values.tolist()).split()))))\n",
    "        \n",
    "        for i,word in enumerate(list(self.dictionary.keys())):\n",
    "            one_hot_encoding = torch.zeros((1,len(self.dictionary)))\n",
    "            one_hot_encoding[0,i] = 1\n",
    "            self.dictionary[word] = one_hot_encoding        \n",
    "        \n",
    "        self.dataset = self.dataset.values\n",
    "                                \n",
    "    def __getitem__(self,index):\n",
    "        if not isinstance(index,slice):\n",
    "            row = self.dataset[index,:]\n",
    "            img = Image.open(os.path.join(self.IMAGE_PATH,row[0])) \n",
    "            if self.transforms:\n",
    "                img = self.transforms(img)\n",
    "            img.unsqueeze_(0)\n",
    "            \n",
    "            text = row[1]\n",
    "            text_encoded = self.dictionary[text[0]]\n",
    "            for word in text[1:]:\n",
    "                text_encoded = torch.cat((text_encoded,self.dictionary[word]),0)\n",
    "            text_encoded.unsqueeze_(0)\n",
    "            \n",
    "            labels = text_encoded.nonzero()[:,2].reshape(-1,self.sentence_len)\n",
    "                        \n",
    "            return img,text_encoded,labels\n",
    "            \n",
    "            \n",
    "        images = []\n",
    "        captions = []\n",
    "        for row in self.dataset[index,:]:\n",
    "            # load images\n",
    "            img = Image.open(os.path.join(self.IMAGE_PATH,row[0])) \n",
    "            if self.transforms:\n",
    "                img = self.transforms(img)\n",
    "            img.unsqueeze_(0)\n",
    "            images.append(img)\n",
    "        \n",
    "            # load captions\n",
    "            text = row[1]\n",
    "            text_encoded = self.dictionary[text[0]]\n",
    "            for word in text[1:]:\n",
    "                text_encoded = torch.cat((text_encoded,self.dictionary[word]),0)\n",
    "            text_encoded.unsqueeze_(0)\n",
    "            captions.append(text_encoded)\n",
    "            \n",
    "        images = torch.cat(images,0)\n",
    "        captions = torch.cat(captions,0)\n",
    "        labels = (captions !=0 ).nonzero()[:,2].reshape(-1,self.sentence_len)\n",
    "                     \n",
    "        return images,captions,labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.flickr8000_subset at 0x2b702af8848>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flickr8000_subset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
